import os
import Levenshtein

def calculate_ned(predicted_text, ground_truth_text):
    """
    Calculate the Normalized Edit Distance (NED) between the predicted text and the ground truth text.
    
    Parameters:
    predicted_text (str): The ABC notation text generated by the model.
    ground_truth_text (str): The actual ABC notation text.
    
    Returns:
    float: The normalized edit distance score (between 0 and 1, where 0 indicates a perfect match).
    """
    # Calculate edit distance
    edit_distance = Levenshtein.distance(predicted_text, ground_truth_text)
    
    # Normalize using the length of the ground truth text
    ned = edit_distance / max(len(ground_truth_text), 1)  # Avoid division by zero
    
    return ned

def read_abc_file(file_path):
    """
    Read the contents of an ABC notation file.
    
    Parameters:
    file_path (str): The path to the file.
    
    Returns:
    str: The content of the file as a string.
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()
        return content
    except FileNotFoundError:
        print(f"Error: File {file_path} not found.")
        return None
    except Exception as e:
        print(f"Error reading file: {e}")
        return None

def evaluate_abc_files(predicted_file, ground_truth_file):
    """
    Evaluate the similarity between two ABC files.
    
    Parameters:
    predicted_file (str): Path to the predicted file.
    ground_truth_file (str): Path to the ground truth file.
    
    Returns:
    dict: A dictionary containing the evaluation results.
    """
    # Read file contents
    predicted_text = read_abc_file(predicted_file)
    ground_truth_text = read_abc_file(ground_truth_file)
    
    if predicted_text is None or ground_truth_text is None:
        return None
    
    # Calculate NED
    ned_score = calculate_ned(predicted_text, ground_truth_text)
    
    # Return results
    result = {
        'predicted_file': predicted_file,
        'ground_truth_file': ground_truth_file,
        'predicted_length': len(predicted_text),
        'ground_truth_length': len(ground_truth_text),
        'edit_distance': Levenshtein.distance(predicted_text, ground_truth_text),
        'ned_score': ned_score,
        'accuracy': 1 - ned_score  # Accuracy = 1 - NED
    }
    
    return result

# Example usage
if __name__ == "__main__":
    # File paths
    predicted_file = "results/QmQnxTXwmezR5ktFw5vz4CL7sz4NLJrGtGd2ZUZsyi4vaG.abc"
    ground_truth_file = "gt_xml/QmQnxTXwmezR5ktFw5vz4CL7sz4NLJrGtGd2ZUZsyi4vaG.abc"
    
    # Evaluation
    result = evaluate_abc_files(predicted_file, ground_truth_file)
    
    if result:
        print("Evaluation results:")
        print(f"Predicted file: {result['predicted_file']}")
        print(f"Ground truth file: {result['ground_truth_file']}")
        print(f"Predicted text length: {result['predicted_length']}")
        print(f"Ground truth text length: {result['ground_truth_length']}")
        print(f"Edit distance: {result['edit_distance']}")
        print(f"Normalized Edit Distance (NED): {result['ned_score']:.4f}")
        print(f"Accuracy: {result['accuracy']:.4f}")
    else:
        print("Evaluation failed, please check the file paths.")